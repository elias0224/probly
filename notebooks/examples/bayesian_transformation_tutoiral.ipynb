{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e484b5f",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Statistics and Bayesian Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211f574",
   "metadata": {},
   "source": [
    "## 1. Bayes Theorem\n",
    "\n",
    "Bayes’ theorem describes how to update the probability of a **hypothesis (A)** given some **evidence (B)**.\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(A|B)$: Posterior — probability of the hypothesis after seeing evidence\n",
    "- $P(B|A)$: Likelihood — probability of observing evidence if H is true\n",
    "- $P(A)$: Prior — our initial belief about H\n",
    "- $P(B)$: Evidence — total probability of the evidence\n",
    "\n",
    "Let's do an example! \n",
    "\n",
    "We want to estimate the probability that a developer writes high-quality code (A) given that they train their coding skills frequently (B)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f77ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "P_high_quality = 0.3  # Prior - P(A)\n",
    "P_low_quality = 1 - P_high_quality\n",
    "\n",
    "P_training_given_high = 0.8  # Likelihood - P(B|A)\n",
    "P_training_given_low = 0.4  # P(B|not A)\n",
    "\n",
    "# Total probability of attending training (Evidence - P(B))\n",
    "P_training = (P_training_given_high * P_high_quality) + (P_training_given_low * P_low_quality)\n",
    "\n",
    "# Bayes' theorem\n",
    "P_high_given_training = (P_training_given_high * P_high_quality) / P_training\n",
    "\n",
    "print(\n",
    "    f\"There's a {P_high_given_training * 100:.2f}% chance that \"\n",
    "    f\"a developer with frequent training writes high-quality code.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d558b267",
   "metadata": {},
   "source": [
    "Lets visualize it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0763f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle(\"Effect of Frequent Training on Code Quality\")\n",
    "\n",
    "# High Quality Code\n",
    "categories = [\"Prior (Any Programmer)\", \"Posterior (Attends Training)\"]\n",
    "probabilities = [P_high_quality, P_high_given_training]\n",
    "ax[0].bar(categories, probabilities, color=[\"crimson\", \"brown\"], edgecolor=\"k\")\n",
    "ax[0].set_ylim(0, 1)\n",
    "ax[0].set_ylabel(\"P(High-Quality Code)\")\n",
    "\n",
    "# Low Quality\n",
    "categories = [\"Prior (Any Programmer)\", \"Posterior (Attends no Training)\"]\n",
    "P_no_training = 1 - P_training\n",
    "P_no_training_given_low = 1 - P_training_given_low\n",
    "P_low_given_training = (P_no_training_given_low * P_low_quality) / P_no_training\n",
    "probabilities = [P_low_quality, P_low_given_training]\n",
    "ax[1].bar(categories, probabilities, color=[\"magenta\", \"darkmagenta\"], edgecolor=\"k\")\n",
    "ax[1].set_ylim(0, 1)\n",
    "ax[1].set_ylabel(\"P(Low-Quality Code)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d73989f",
   "metadata": {},
   "source": [
    "## 2. Bayesian Inference\n",
    "Bayesian Inference is a generalization of Bayes Theorem to more complex problems, meaning scenarios with multiple parameters instead of one measurable quantity.\n",
    "\n",
    "For parameters $\\theta$ = ($\\theta_1$, $\\theta_1$, …, $\\theta_n$):\n",
    "\n",
    "$$\n",
    "P(θ|D) \\sim P(D|θ)P(θ)\n",
    "$$\n",
    "\n",
    "- $P(θ)$: Prior (joint distribution)\n",
    "- $P(D|θ)$: Likelihood\n",
    "- $P(θ|D)$: Posterior — updated beliefs after observing data\n",
    "\n",
    "Because the parameters $\\theta$ are independent we can rewrite it as:\n",
    "\n",
    "$$\n",
    "P(\\theta_1, \\theta_1, …, \\theta_n|D) \\sim P(D|\\theta_1, \\theta_1, …, \\theta_n)P(\\theta_1)P(\\theta_2)...P(\\theta_n)\n",
    "$$\n",
    "\n",
    "For that purpose we give an example where we want to estimate the mean and the standard deviation of a normal distribution from synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Generate Synthetic Data\n",
    "np.random.seed(42)\n",
    "true_mu = 5\n",
    "true_sigma = 2\n",
    "data = np.random.normal(true_mu, true_sigma, size=100)\n",
    "\n",
    "# Visualize the Synthetic Data\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.title(\"Synthetic Data\")\n",
    "\n",
    "plt.hist(data, bins=20, edgecolor=\"k\", density=True, color=\"lightblue\")\n",
    "\n",
    "x = np.linspace(-5, 15, 100)\n",
    "plt.plot(x, norm.pdf(x, true_mu, true_sigma), \"k\", ls=\"dashed\", linewidth=2, label=\"Truth\")\n",
    "\n",
    "plt.xlim(0, 10)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"x\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219220b3",
   "metadata": {},
   "source": [
    "To estimate the posterior distribution of our model parameters we need to define the likelihood and certain prior beliefs.\n",
    "\n",
    "1. **Likelihood:**  \n",
    "    $$x_i \\sim \\mathcal{N}(\\mu, \\sigma)$$\n",
    "    The likelihood measures how plausible the observed data is for different values of the parameters. And as we assume our data to be normal distributed the likelihood is aswell.\n",
    "2. **Priors:**\n",
    "\n",
    "    $$\\mu \\sim \\mathcal{N}(0, 10)$$\n",
    "    Before observing any data, we believe that the mean is likely near 0 but has also a chance to be something else depending on the given standard deviation. The larger the standard deviation the more uncertain we are about its prior belief.\n",
    "\n",
    "    $$\\sigma \\sim \\text{Uniform}(0, 10)$$\n",
    "    Before observing any data, we believe all values of the standard deviation between 0 and 10 are equally likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6970a957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "\n",
    "def log_prior(mu: float, sigma: float) -> float:\n",
    "    if sigma <= 0:\n",
    "        return -np.inf\n",
    "    prior_mu = norm.logpdf(mu, 0, 10)\n",
    "    prior_sigma = uniform.logpdf(sigma, 0, 10)\n",
    "    return prior_mu + prior_sigma\n",
    "\n",
    "\n",
    "def log_likelihood(mu: float, sigma: float, data: np.ndarray) -> float:\n",
    "    return np.sum(norm.logpdf(data, mu, sigma))\n",
    "\n",
    "\n",
    "def log_posterior(mu: float, sigma: float, data: np.ndarray) -> float:\n",
    "    return log_likelihood(mu, sigma, data) + log_prior(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e518792",
   "metadata": {},
   "source": [
    "We’ll compute the posterior:\n",
    "    $$P(\\mu, \\sigma | \\text{data}) \\propto P(\\text{data} | \\mu, \\sigma) \\, P(\\mu) \\, P(\\sigma)$$\n",
    "\n",
    "As we use log likelihoods and log priors this transforms to the log posterior:\n",
    "    $$\\mathcal{L}_P(\\mu, \\sigma | \\text{data}) \\propto \\mathcal{L}_P(\\text{data} | \\mu, \\sigma) + \\mathcal{L}_P(\\mu) + \\mathcal{N}_P(\\sigma)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d9d8d",
   "metadata": {},
   "source": [
    "No we evaluate the posterior on a grid of $(\\mu, \\sigma)$ values to visualize how Bayesian inference updates our beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Ranges\n",
    "mu_vals = np.linspace(0, 10, 100)\n",
    "sigma_vals = np.linspace(0.1, 5, 100)\n",
    "\n",
    "# Empty Posterior Grid\n",
    "posterior = np.zeros((len(mu_vals), len(sigma_vals)))\n",
    "\n",
    "# Parameter Sampling\n",
    "for i, mu in enumerate(mu_vals):\n",
    "    for j, sigma in enumerate(sigma_vals):\n",
    "        posterior[i, j] = log_posterior(mu, sigma, data)\n",
    "\n",
    "# Converting back from a log posterior and normalize it to get a proper discrete probabiltity distribution\n",
    "posterior = np.exp(posterior - np.max(posterior))\n",
    "posterior /= np.sum(posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced9eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the 2D Posterior\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.contourf(sigma_vals, mu_vals, posterior, levels=30, cmap=\"viridis\")\n",
    "plt.xlabel(r\"$\\sigma$\")\n",
    "plt.ylabel(r\"$\\mu$\")\n",
    "plt.title(r\"Posterior Distribution\")\n",
    "plt.colorbar(label=\"Posterior Density\")\n",
    "plt.scatter(true_sigma, true_mu, color=\"red\", label=\"True value\")\n",
    "plt.xlim(true_sigma - 1, true_sigma + 1)\n",
    "plt.ylim(true_mu - 1, true_mu + 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ad0200",
   "metadata": {},
   "source": [
    "The final step is to marginalize over the joint posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4922a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "marginal_mu = posterior.sum(axis=1)  # sum up the standard deviation\n",
    "marginal_sigma = posterior.sum(axis=0)  # sum up the mean\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax[0].plot(mu_vals, marginal_mu)\n",
    "ax[0].axvline(true_mu, color=\"r\", linestyle=\"--\")\n",
    "ax[0].set_xlim(true_mu - 1, true_mu + 1)\n",
    "ax[0].set_title(r\"Marginal Posterior of $\\mu$\")\n",
    "\n",
    "\n",
    "ax[1].plot(sigma_vals, marginal_sigma)\n",
    "ax[1].axvline(true_sigma, color=\"r\", linestyle=\"--\", label=\"Truth\")\n",
    "ax[1].set_xlim(true_sigma - 1, true_sigma + 1)\n",
    "ax[1].set_title(r\"Marginal Posterior of $\\sigma$\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d493c9f5",
   "metadata": {},
   "source": [
    "Finally we compute the posterior mean for $\\mu$ and $\\sigma$. This can be doner either by calculate the expection value, i.e. the weighted average or by taking the value with highest probabilty. However latter can be misleading as the posterior distirbution might be non-symmetric (skewed or multiple peaks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91a52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_est = np.sum(mu_vals * marginal_mu)\n",
    "sigma_est = np.sum(sigma_vals * marginal_sigma)\n",
    "\n",
    "print(\"Posterior mean estimates:\")\n",
    "print(rf\"mean ~ {mu_est:.2f}, standard deviation ~ {sigma_est:.2f}\")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.title(\"Synthetic Data\")\n",
    "\n",
    "plt.hist(data, bins=20, edgecolor=\"k\", density=True, color=\"lightblue\")\n",
    "plt.plot(x, norm.pdf(x, true_mu, true_sigma), \"k\", ls=\"dashed\", linewidth=1, label=\"Truth\")\n",
    "plt.plot(x, norm.pdf(x, mu_est, sigma_est), \"k\", ls=\"solid\", linewidth=2, label=\"Bayesian Inference\")\n",
    "plt.xlim(0, 10)\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041135d5",
   "metadata": {},
   "source": [
    "## 3. Bayesian Transformation - probly\n",
    "\n",
    "Finally the question arises, how can we apply this knowledge to make models uncertainty aware?\n",
    "\n",
    "The motivation behind this all is, that classical neural output single predictions. But how uncertain are these predictions? Instead of simply minimzing the loss function to adjust weights in the model and hence make predictions, Bayesian layers will learn distributions (simply speaking).\n",
    "\n",
    "Lets see how **probly** does it. Again we start by creating synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93df47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Generate toy data: y = sin(x) + noise\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = np.sin(x) + 0.1 * np.random.randn(*x.shape)\n",
    "\n",
    "# Convert to torch tensors\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(x, y)\n",
    "plt.title(\"Synthetic Regression Data\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39b08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initializes the neural network layers and inherits basic nn.Module functionality.\"\"\"\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, 50)\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.fc3 = nn.Linear(50, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# Instantiate model, loss, optimizer\n",
    "model = ClassicNN()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for _ in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_tensor)\n",
    "    loss = criterion(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predictions\n",
    "y_pred = model(x_tensor).detach().numpy()\n",
    "\n",
    "# Plot\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot(x, y_pred, color=\"red\", label=\"Predictions\")\n",
    "plt.title(\"Classic NN Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666ba61",
   "metadata": {},
   "source": [
    "Lets convert each layer of the Network into bayesian layer from probly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb2292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.transformation import bayesian\n",
    "\n",
    "model = bayesian(ClassicNN())\n",
    "model.train()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for _ in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(x_tensor)\n",
    "    loss = nn.MSELoss()(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Predictions\n",
    "y_pred = model(x_tensor).detach().numpy()\n",
    "# Plot\n",
    "plt.scatter(x, y, label=\"Data\")\n",
    "plt.plot(x, y_pred, color=\"red\", label=\"Predictions\")\n",
    "plt.title(\"Classic NN Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from probly.layers.torch import BayesConv2d, BayesLinear\n",
    "\n",
    "\n",
    "def posterior_summary(model: nn.Module) -> list[dict[str, float]]:\n",
    "    summaries = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (BayesLinear, BayesConv2d)):\n",
    "            # posterior mean\n",
    "            weight_mu = module.weight_mu.detach().cpu()\n",
    "            # posterior std is stored via a transformed parameter `rho`\n",
    "            weight_sigma = torch.log1p(torch.exp(module.weight_rho.detach().cpu()))\n",
    "            summary = {\n",
    "                \"layer\": name or module.__class__.__name__,\n",
    "                \"weight_mu_mean\": float(weight_mu.mean()),\n",
    "                \"weight_mu_std\": float(weight_mu.std()),\n",
    "                \"weight_post_sigma_mean\": float(weight_sigma.mean()),\n",
    "                \"weight_post_sigma_std\": float(weight_sigma.std()),\n",
    "            }\n",
    "            if getattr(module, \"bias\", False):\n",
    "                bias_mu = module.bias_mu.detach().cpu()\n",
    "                bias_sigma = torch.log1p(torch.exp(module.bias_rho.detach().cpu()))\n",
    "                summary.update(\n",
    "                    {\n",
    "                        \"bias_mu_mean\": float(bias_mu.mean()),\n",
    "                        \"bias_mu_std\": float(bias_mu.std()),\n",
    "                        \"bias_post_sigma_mean\": float(bias_sigma.mean()),\n",
    "                        \"bias_post_sigma_std\": float(bias_sigma.std()),\n",
    "                    },\n",
    "                )\n",
    "            summaries.append(summary)\n",
    "    return summaries\n",
    "\n",
    "\n",
    "for s in posterior_summary(model):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_posterior_samples(\n",
    "    model: nn.Module,\n",
    "    x_tensor: torch.Tensor,\n",
    "    n_samples: int = 500,\n",
    "    device: torch.device = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Return numpy array of predictive samples with shape (n_samples, batch, output_dim).\n",
    "\n",
    "    The model's Bayesian layers sample internally on each forward pass,\n",
    "    so multiple forwards approximate the posterior predictive.\n",
    "    \"\"\"\n",
    "    params = list(model.parameters())\n",
    "    device = device or (params[0].device if len(params) > 0 else torch.device(\"cpu\"))\n",
    "\n",
    "    model.eval()\n",
    "    x = x_tensor.to(device)\n",
    "    samples = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            out = model(x)  # stochastic forward; Bayes layers resample weights each call\n",
    "            samples.append(out.detach().cpu().numpy())\n",
    "    samples = np.stack(samples, axis=0)\n",
    "    return samples\n",
    "\n",
    "\n",
    "# --- Run sampling ---\n",
    "n_samples = 500\n",
    "samples = predictive_posterior_samples(model, x_tensor, n_samples=n_samples)\n",
    "\n",
    "# Compute summary statistics\n",
    "mean = samples.mean(axis=0)  # shape (batch, output_dim)\n",
    "std = samples.std(axis=0)\n",
    "q05 = np.quantile(samples, 0.05, axis=0)\n",
    "q95 = np.quantile(samples, 0.95, axis=0)\n",
    "\n",
    "# Squeeze to 1D if outputs are single-valued per x\n",
    "y_mean = mean.squeeze()\n",
    "y_std = std.squeeze()\n",
    "y_low = q05.squeeze()\n",
    "y_high = q95.squeeze()\n",
    "\n",
    "# --- Plot predictive mean with 90% interval ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x, y, s=10, label=\"data\", alpha=0.6)\n",
    "plt.plot(x, y_mean, color=\"red\", label=\"predictive mean\")\n",
    "plt.fill_between(x.flatten(), y_low, y_high, color=\"red\", alpha=0.2, label=\"90% CI\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Predictive posterior: mean and 90% interval\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7409f281",
   "metadata": {},
   "source": [
    "Comment: We see that the model is now uncertainty aware. Note, that the uncertainty decreases the narrower the data points spread."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
