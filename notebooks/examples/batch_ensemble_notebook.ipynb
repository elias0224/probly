{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Batch Ensemble Networks\n",
    "\n",
    "This notebook functions the same as the MIMO Notebook but implements a Batch Ensemble MLP instead.\n",
    "### Introduction\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## 1. Quick Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)  # oqa E702\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## 2. Toy dataset (classification variant)\n",
    "\n",
    "Make use of a simple moon dataset from sklearn to have fast training time. Splitting the dataset in 80% training data and 20% evalulation data and making the data ready in batch sizes of 128 for training and evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 24000 Val size: 6000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=30000, noise=0.2, random_state=seed)\n",
    "X = X.astype(\"float32\")\n",
    "y = y.astype(\"int64\")\n",
    "\n",
    "split = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n",
    "\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"Train size:\", len(train_ds), \"Val size:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## 3. Baseline model (standard MLP)\n",
    "\n",
    "Creating a MLP to use as a base model for the ensemble to have a fast training time.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int = 2, hidden: int = 128, out_dim: int = 2) -> None:\n",
    "        \"\"\"MLP with two hidden layers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Simple forward.\"\"\"\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "m = MLP().to(device)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35323312",
   "metadata": {},
   "source": [
    "## 4. Define Batch Ensemble Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dcd06d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchEnsembleLinear(nn.Module):\n",
    "    \"\"\"A fully connected layer with BatchEnsemble fast weights.\"\"\"\n",
    "    def __init__(self, in_dim, out_dim, M):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        # Shared base weight & bias\n",
    "        self.W = nn.Parameter(torch.randn(out_dim, in_dim) * 0.1)\n",
    "        self.b = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "        # Fast weights (rank-1 factors)\n",
    "        self.r = nn.Parameter(torch.randn(M, in_dim))   # input direction\n",
    "        self.s = nn.Parameter(torch.randn(M, out_dim))  # output direction\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, in_features] or [B, M, in_features]\n",
    "        if x.dim() == 2:\n",
    "            # first layer: [B, in_features] -> [B, M, out_features]\n",
    "            fast_in = torch.einsum(\"bi,mi->bm\", x, self.r)\n",
    "            fast = fast_in.unsqueeze(-1) * self.s.unsqueeze(0)\n",
    "            shared = x @ self.W.t() + self.b\n",
    "            shared = shared.unsqueeze(1)\n",
    "            out = shared + fast\n",
    "        elif x.dim() == 3:\n",
    "            # later layers: [B, M, in_features]\n",
    "            fast_in = (x * self.r.unsqueeze(0)).sum(-1)  # [B, M]\n",
    "            fast = fast_in.unsqueeze(-1) * self.s.unsqueeze(0)  # [B, M, out_features]\n",
    "            shared = x @ self.W.t() + self.b  # broadcasting works\n",
    "            out = shared + fast\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape {x.shape}\")\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## 5. Batch Ensemble wrapper: make Batch Ensemble version of the MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchEnsemble(\n",
      "  (input_layer): BatchEnsembleLinear()\n",
      "  (body): Sequential(\n",
      "    (0): ReLU()\n",
      "  )\n",
      "  (fc): BatchEnsembleLinear()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BatchEnsemble(nn.Module):\n",
    "    def __init__(self, base_hidden=128, in_dim=2, out_dim=2, M=3):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        \n",
    "        self.input_layer = BatchEnsembleLinear(in_dim, base_hidden, M)\n",
    "\n",
    "        self.body = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = BatchEnsembleLinear(base_hidden, out_dim, M)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch, in_dim]\n",
    "        Returns: [batch, M, out_dim]\n",
    "        \"\"\"\n",
    "        h = self.input_layer(x)       # [B, M, H]\n",
    "        h = self.body(h)              # apply activation\n",
    "        out = self.fc(h)              # [B, M, O]\n",
    "        return out\n",
    "\n",
    "\n",
    "be_model = BatchEnsemble(M=3).to(device)\n",
    "print(be_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93787cf9",
   "metadata": {},
   "source": [
    "## 5. Comparison between Ensemble and MIMO-Idea\n",
    "\n",
    "Training an Ensemble and a MIMO to compare acc, loss, ece, MI(disagreement), forward calls.\n",
    "The number of subnetworks is four as recommended in [TRAINING INDEPENDENT SUBNETWORKS FOR ROBUST\n",
    "PREDICTION](https://openreview.net/pdf?id=OGg9XnKxFAH)\n",
    " \n",
    "Setup:\n",
    "\n",
    "dataset with 24k datapoints to train and 6k datapoints for evalulation <br>\n",
    "K members in ensemble = 3 <br>\n",
    "K subnetworks in the MIMO = 3 <br> \n",
    "Epochs = 10 <br>\n",
    "lr = 1e-3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bfc9a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small experiments: train MIMO and an ensemble with comparable capacity\n",
    "k = 3\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "def softmax_np(logits: np.ndarray, axis: int = -1) -> np.ndarray:\n",
    "    e = np.exp(logits - logits.max(axis=axis, keepdims=True))\n",
    "    return e / e.sum(axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def entropy_np(probs: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:\n",
    "    p = np.clip(probs, eps, 1.0)\n",
    "    return -np.sum(p * np.log(p), axis=axis)\n",
    "\n",
    "\n",
    "def ece_score(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15) -> float:\n",
    "    # probs: (N, C) predictive mean probs; labels: (N,)\n",
    "    confs = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    n = len(labels)\n",
    "    for i in range(n_bins):\n",
    "        mask = (confs >= bins[i]) & (confs < bins[i + 1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        acc = (preds[mask] == labels[mask]).mean()\n",
    "        conf = confs[mask].mean()\n",
    "        ece += (mask.sum() / n) * abs(conf - acc)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "def reliability_diagram(probs: np.ndarray, labels: np.ndarray, n_bins: int = 15, ax: None = None) -> plt.Axes:\n",
    "    confs = probs.max(axis=1)\n",
    "    preds = probs.argmax(axis=1)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    accs = []\n",
    "    avg_confs = []\n",
    "    counts = []\n",
    "    for i in range(n_bins):\n",
    "        mask = (confs >= bins[i]) & (confs < bins[i + 1])\n",
    "        counts.append(mask.sum())\n",
    "        if mask.sum() == 0:\n",
    "            accs.append(np.nan)\n",
    "            avg_confs.append(np.nan)\n",
    "        else:\n",
    "            accs.append((preds[mask] == labels[mask]).mean())\n",
    "            avg_confs.append(confs[mask].mean())\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    ax.plot(bin_centers, accs, marker=\"o\", label=\"accuracy per bin\")\n",
    "    ax.plot([0, 1], [0, 1], \"--\", color=\"gray\")\n",
    "    ax.set_xlabel(\"Confidence\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_title(\"Reliability diagram\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "# Train a normal ensemble of K independently initialized MLPs\n",
    "def train_ensemble(\n",
    "    base_cls: nn.Module,\n",
    "    k: int,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = epochs,\n",
    "    lr: float = 1e-3,\n",
    ") -> list:\n",
    "    models = []\n",
    "    ensemble_forward_calls = 0\n",
    "    for _ in range(k):\n",
    "        print(f\"\\nTraining ensemble member {_ + 1}/{k}\")\n",
    "        m_k = base_cls().to(device)\n",
    "        opt = optim.Adam(m_k.parameters(), lr=lr)\n",
    "        lossfn = nn.CrossEntropyLoss()\n",
    "        for epoch in range(epochs):\n",
    "            m_k.train()\n",
    "            for xb, yb in train_loader:\n",
    "                x = xb.to(device).float()\n",
    "                y = yb.to(device).long()\n",
    "                opt.zero_grad()\n",
    "                ensemble_forward_calls += 1\n",
    "                out = m_k(x)\n",
    "                loss = lossfn(out, y)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "        models.append(m_k)\n",
    "\n",
    "    print(f\"Ensemble total forward calls: {ensemble_forward_calls}\")\n",
    "    return models\n",
    "\n",
    "\n",
    "def train_batchensemble(\n",
    "    be_model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    epochs: int = epochs,\n",
    "    lr: float = lr,\n",
    ") -> nn.Module:\n",
    "    \"\"\"\n",
    "    Train a BatchEnsemble model.\n",
    "    be_model: BatchEnsemble instance\n",
    "    \"\"\"\n",
    "    opt = torch.optim.Adam(be_model.parameters(), lr=lr)\n",
    "    lossfn = torch.nn.CrossEntropyLoss()\n",
    "    forward_calls = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        be_model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            x = xb.to(device).float()\n",
    "            y = yb.to(device).long()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            forward_calls += 1\n",
    "            out = be_model(x)  # [batch, M, out_dim]\n",
    "\n",
    "            # Mean over ensemble members for loss\n",
    "            out_mean = out.mean(dim=1)  # [batch, out_dim]\n",
    "            loss = lossfn(out_mean, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    print(f\"BatchEnsemble total forward calls: {forward_calls}\")\n",
    "    return be_model\n",
    "\n",
    "\n",
    "# Evaluation helpers\n",
    "def eval_ensemble_models(models: list, x_np: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # returns mean_probs (N, C), member_probs (N, K, C)\n",
    "    x = torch.from_numpy(x_np).to(device).float()\n",
    "    member_probs = []\n",
    "    for m in models:\n",
    "        m.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = m(x).cpu().numpy()\n",
    "            member_probs.append(softmax_np(logits))\n",
    "    member_probs = np.stack(member_probs, axis=1)  # (N, K, C)\n",
    "    mean_probs = member_probs.mean(axis=1)\n",
    "    return mean_probs, member_probs\n",
    "\n",
    "\n",
    "def eval_batchensemble_model(\n",
    "    be_model: nn.Module,\n",
    "    x_np: np.ndarray\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate a trained BatchEnsemble model.\n",
    "    Returns:\n",
    "        mean_probs: [N, C]\n",
    "        member_probs: [N, M, C]\n",
    "    \"\"\"\n",
    "    x = torch.from_numpy(x_np).to(device).float()\n",
    "    be_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = be_model(x).cpu().numpy()  # [N, M, C]\n",
    "    member_probs = softmax_np(logits, axis=-1)\n",
    "    mean_probs = member_probs.mean(axis=1)\n",
    "    return mean_probs, member_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050eb98",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training and tracking time needed to train and displaying the results in differetn plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0831a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ensemble member 1/3\n",
      "Epoch 1/10, Loss: 0.2620\n",
      "Epoch 2/10, Loss: 0.0527\n",
      "Epoch 3/10, Loss: 0.1919\n",
      "Epoch 4/10, Loss: 0.0583\n",
      "Epoch 5/10, Loss: 0.0346\n",
      "Epoch 6/10, Loss: 0.0740\n",
      "Epoch 7/10, Loss: 0.0430\n",
      "Epoch 8/10, Loss: 0.0722\n",
      "Epoch 9/10, Loss: 0.0916\n",
      "Epoch 10/10, Loss: 0.0698\n",
      "\n",
      "Training ensemble member 2/3\n",
      "Epoch 1/10, Loss: 0.1035\n",
      "Epoch 2/10, Loss: 0.1127\n",
      "Epoch 3/10, Loss: 0.1962\n",
      "Epoch 4/10, Loss: 0.0441\n",
      "Epoch 5/10, Loss: 0.0823\n",
      "Epoch 6/10, Loss: 0.0956\n",
      "Epoch 7/10, Loss: 0.0649\n",
      "Epoch 8/10, Loss: 0.1649\n",
      "Epoch 9/10, Loss: 0.0854\n",
      "Epoch 10/10, Loss: 0.1114\n",
      "\n",
      "Training ensemble member 3/3\n",
      "Epoch 1/10, Loss: 0.0909\n",
      "Epoch 2/10, Loss: 0.0889\n",
      "Epoch 3/10, Loss: 0.1157\n",
      "Epoch 4/10, Loss: 0.1122\n",
      "Epoch 5/10, Loss: 0.1417\n",
      "Epoch 6/10, Loss: 0.0681\n",
      "Epoch 7/10, Loss: 0.0268\n",
      "Epoch 8/10, Loss: 0.0825\n",
      "Epoch 9/10, Loss: 0.0891\n",
      "Epoch 10/10, Loss: 0.0462\n",
      "Ensemble total forward calls: 5640\n",
      "Epoch 1/10, Loss: 0.2637\n",
      "Epoch 2/10, Loss: 0.1423\n",
      "Epoch 3/10, Loss: 0.2917\n",
      "Epoch 4/10, Loss: 0.1985\n",
      "Epoch 5/10, Loss: 0.1700\n",
      "Epoch 6/10, Loss: 0.0686\n",
      "Epoch 7/10, Loss: 0.1220\n",
      "Epoch 8/10, Loss: 0.0504\n",
      "Epoch 9/10, Loss: 0.0967\n",
      "Epoch 10/10, Loss: 0.0234\n",
      "BatchEnsemble total forward calls: 1880\n",
      "BatchEnsemble training time: 11.90s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ensemble_models = train_ensemble(MLP, k, train_loader, epochs=epochs, lr=lr)\n",
    "ensemble_time = time.time() - start\n",
    "\n",
    "# Train BatchEnsemble model for fairness\n",
    "start = time.time()\n",
    "train_batchensemble(be_model, train_loader, epochs=epochs, lr=lr)\n",
    "be_time = time.time() - start\n",
    "print(f\"BatchEnsemble training time: {be_time:.2f}s\")\n",
    "\n",
    "# Eval on validation set\n",
    "X_val_np = X_val.astype(\"float32\")\n",
    "y_val_np = y_val.astype(\"int64\")\n",
    "\n",
    "be_mean, be_members = eval_batchensemble_model(be_model, X_val_np)  # [N, C], [N, M, C]\n",
    "ens_mean, ens_members = eval_ensemble_models(ensemble_models, X_val_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660dfca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probly (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
